


Traceback (most recent call last):
  File "/home/narisam/sp23-nmep-hw1/main.py", line 301, in <module>
    main(config)
  File "/home/narisam/sp23-nmep-hw1/main.py", line 116, in main
    train_acc1, train_loss = train_one_epoch(config, model, criterion, data_loader_train, optimizer, epoch)
  File "/home/narisam/sp23-nmep-hw1/main.py", line 172, in train_one_epoch
    optimizer.step()
  File "/home/narisam/miniconda3/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 68, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/narisam/miniconda3/lib/python3.10/site-packages/torch/optim/optimizer.py", line 140, in wrapper
    out = func(*args, **kwargs)
  File "/home/narisam/miniconda3/lib/python3.10/site-packages/torch/autograd/grad_mode.py", line 27, in decorate_context
    return func(*args, **kwargs)
  File "/home/narisam/miniconda3/lib/python3.10/site-packages/torch/optim/adamw.py", line 162, in step
    adamw(params_with_grad,
  File "/home/narisam/miniconda3/lib/python3.10/site-packages/torch/optim/adamw.py", line 219, in adamw
    func(params,
  File "/home/narisam/miniconda3/lib/python3.10/site-packages/torch/optim/adamw.py", line 274, in _single_tensor_adamw
    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)
KeyboardInterrupt